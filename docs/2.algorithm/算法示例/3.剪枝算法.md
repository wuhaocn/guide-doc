## [决策树剪枝算法](https://www.cnblogs.com/ritchiewang/p/5767387.html)

**算法目的**：决策树的剪枝是为了简化决策树模型，避免过拟合。

**算法基本思路**：减去决策树模型中的一些子树或者叶结点，并将其根结点作为新的叶结点，从而实现模型的简化。

**模型损失函数**

1. 变量预定义：|T|表示树 T 的叶节点个数，t 表示树 T 的叶节点，同时，Nt 表示该叶节点的样本点个数，其中属于 k 类的样本点有 Ntk 个，K 表示类别的个数，Ht(T)为叶结点 t 上的经验熵，α≥0 为参数
2. 损失函数：Ca(T)=∑t=1|T|NtHt(T)+α|T|
3. 简化表示：C(T)=∑t=1|T|NtHt(T)
4. 经验熵：Ht(T)=−∑kKNtkNtlogNtkNt
   损失函数简化形式：Ca(T)=C(T)+α|T|

这里的经验熵反应了一个叶结点中的分类结果的混乱程度，经验熵越大，说明该叶节点所对应的分类结果越混乱，也就是说分类结果中包含了较多的类别，表明该分支的分类效果较差。所以，损失函数越大，说明模型的分类效果越差。
决策树的剪枝通常分为两种，即预剪枝、后剪枝。
预剪枝是在决策树生成过程中，对树进行剪枝，提前结束树的分支生长。
后剪枝是在决策树生长完成之后，对树进行剪枝，得到简化版的决策树。
下面的算法，是**后剪枝**的实现步骤。

### 算法步骤：

输入：生成算法产生的整个树 T，参数 α
输出：修剪后的子树 Tα

1. 计算每个结点的经验熵
2. 递归地从树的叶结点向上回缩
   设一组叶结点回缩到父结点之前与之后的整体树分别为 TB 和 TA，其对应的损失函数值分别是 Cα(TB)与 Cα(TA)，如果 Cα(TA)≤Cα(TB)，即如果进行剪枝，损失函数变小，就进行剪枝，将父结点变为新的叶结点
3. 返回(2)，直至不能继续为止，得到损失函数最小的子树 Tα
